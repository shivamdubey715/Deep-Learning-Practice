{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Specify the input shape and number of classes (e.g., two classes for binary classification)\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 2  # Modify based on your dataset\n",
    "\n",
    "# a. Load in a pre-trained CNN model (e.g., MobileNetV2)\n",
    "base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "# b. Freeze parameters (weights) in modelâ€™s lower convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False  # Freeze base layers\n",
    "\n",
    "# c. Add custom classifier with several layers of trainable parameters\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)       # Global pooling layer\n",
    "x = Dense(1024, activation='relu')(x) # Dense layer\n",
    "x = Dropout(0.5)(x)                   # Dropout layer to reduce overfitting\n",
    "x = Dense(512, activation='relu')(x)  # Additional dense layer\n",
    "predictions = Dense(num_classes, activation='softmax')(x)  # Output layer for classification\n",
    "\n",
    "# Final model with the custom classifier on top of the pre-trained base model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# d. Train classifier layers on training data available for task\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Data Preparation (use ImageDataGenerator for data augmentation)\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, zoom_range=0.15,\n",
    "                                   width_shift_range=0.2, height_shift_range=0.2,\n",
    "                                   shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_dir = \"path/to/train\"  # Specify path to training data\n",
    "val_dir = \"path/to/validation\"  # Specify path to validation data\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(224, 224),\n",
    "                                                    batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_dir, target_size=(224, 224),\n",
    "                                                batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=2)\n",
    "\n",
    "# e. Fine-tune hyperparameters and unfreeze more layers as needed\n",
    "# Unfreeze some layers for fine-tuning\n",
    "for layer in base_model.layers[-20:]:  # Adjust number of layers to unfreeze\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile model with a lower learning rate for fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model again with unfrozen layers\n",
    "fine_tune_history = model.fit(train_generator, validation_data=val_generator, epochs=10, verbose=2)\n",
    "\n",
    "# Display the training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(fine_tune_history.history['accuracy'], label='Fine-tuned Training Accuracy')\n",
    "plt.plot(fine_tune_history.history['val_accuracy'], label='Fine-tuned Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
