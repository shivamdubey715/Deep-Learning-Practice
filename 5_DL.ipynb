{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import itertools\n",
    "\n",
    "# Sample data (a small corpus of text)\n",
    "sentences = [\n",
    "    \"I love machine learning and deep learning\",\n",
    "    \"The continuous bag of words model is great for NLP\",\n",
    "    \"Word embeddings help in finding semantic meaning\",\n",
    "    \"The CBOW model is a popular word2vec method\"\n",
    "]\n",
    "\n",
    "# a. Data Preparation\n",
    "# Tokenize and create vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "vocab_size = len(word2idx) + 1  # +1 for padding\n",
    "window_size = 2  # Number of context words to take around a target word\n",
    "\n",
    "# Convert sentences to sequences of indices\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# b. Generate Training Data\n",
    "def generate_training_data(sequences, window_size, vocab_size):\n",
    "    targets, contexts = [], []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        for idx, word_id in enumerate(sequence):\n",
    "            # Define the context window\n",
    "            start = max(0, idx - window_size)\n",
    "            end = min(len(sequence), idx + window_size + 1)\n",
    "            \n",
    "            # Context words exclude the target word itself\n",
    "            context_words = [sequence[i] for i in range(start, end) if i != idx]\n",
    "            for context_word in context_words:\n",
    "                targets.append(word_id)  # Target word\n",
    "                contexts.append(context_word)  # Context word\n",
    "    \n",
    "    # Convert targets and contexts to numpy arrays\n",
    "    targets = np.array(targets)\n",
    "    contexts = np.array(contexts)\n",
    "    return targets, contexts\n",
    "\n",
    "# Generate the target-context pairs\n",
    "targets, contexts = generate_training_data(sequences, window_size, vocab_size)\n",
    "\n",
    "# c. Train Model\n",
    "embedding_dim = 50  # Dimension of the embedding vector\n",
    "\n",
    "# Define CBOW Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1),  # Embedding layer\n",
    "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),  # Averaging context word embeddings\n",
    "    Dense(vocab_size, activation='softmax')  # Output layer with softmax\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train the model using context words to predict the target word\n",
    "history = model.fit(contexts, targets, epochs=50, batch_size=16, verbose=2)\n",
    "\n",
    "# d. Output\n",
    "# Display embeddings for a sample word\n",
    "word = \"model\"  # Word for which we want to see embedding\n",
    "if word in word2idx:\n",
    "    word_idx = word2idx[word]\n",
    "    embedding = model.layers[0].get_weights()[0][word_idx]\n",
    "    print(f\"Embedding for the word '{word}':\\n{embedding}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not found in vocabulary.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (default, Mar 10 2023, 20:16:38) \n[Clang 14.0.3 (clang-1403.0.22.14.1)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
